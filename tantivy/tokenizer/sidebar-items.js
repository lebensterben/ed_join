initSidebarItems({"constant":[["MAX_TOKEN_LEN","Maximum authorized len (in bytes) for a token."]],"enum":[["Language","Available stemmer languages."]],"struct":[["AlphaNumOnlyFilter","`TokenFilter` that removes all tokens that contain non ascii alphanumeric characters."],["AsciiFoldingFilter","This class converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127 ASCII characters (the \"Basic Latin\" Unicode block) into their ASCII equivalents, if one exists."],["FacetTokenizer","The `FacetTokenizer` process a `Facet` binary representation and emits a token for all of its parent."],["LowerCaser","Token filter that lowercase terms."],["NgramTokenizer","Tokenize the text by splitting words into n-grams of the given size(s)"],["RawTokenizer","For each value of the field, emit a single unprocessed token."],["RemoveLongFilter","`RemoveLongFilter` removes tokens that are longer than a given number of bytes (in UTF-8 representation)."],["SimpleTokenizer","Tokenize the text by splitting on whitespaces and punctuation."],["Stemmer","`Stemmer` token filter. Several languages are supported, see `Language` for the available languages. Tokens are expected to be lowercased beforehand."],["StopWordFilter","`TokenFilter` that removes stop words from a token stream"],["Token","Token"],["TokenizerManager","The tokenizer manager serves as a store for all of the pre-configured tokenizer pipelines."]],"trait":[["BoxedTokenizer","A boxed tokenizer"],["TokenFilter","Trait for the pluggable components of `Tokenizer`s."],["TokenStream","`TokenStream` is the result of the tokenization."],["Tokenizer","`Tokenizer` are in charge of splitting text into a stream of token before indexing."]]});